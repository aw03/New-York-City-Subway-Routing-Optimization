{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a3820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Gurobi, CSV, DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe509ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRecursively scanning directory: .\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\MTA_Subway_Hourly_Ridership__Oct_21_2024_Evening.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\MTA_Subway_Hourly_Ridership__Oct_21_2024_Morning.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\MTA_Subway_Stations_20251204.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\agency.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\calendar.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\calendar_dates.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\linecapacity.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\linelength.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\routes.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\shapes.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\stop_times.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Summary ---\n",
      "Dataset: 'routes'\n",
      "  Shape: 29 rows × 10 columns\n",
      "  Cols:  route_id, agency_id, route_short_name, route_long_name, route_desc, route_type, route_url, route_color, route_text_color, route_sort_order\n",
      "--------------------\n",
      "Dataset: 'nodes_with_ridership'\n",
      "  Shape: 475 rows × 9 columns\n",
      "  Cols:  node_idx, stop_id, stop_name, stop_lon, stop_lat, station_complex_id, ridership_morning, ridership_evening, net_ridership\n",
      "--------------------\n",
      "Dataset: 'stops'\n",
      "  Shape: 1488 rows × 6 columns\n",
      "  Cols:  stop_id, stop_name, stop_lat, stop_lon, location_type, parent_station\n",
      "--------------------\n",
      "Dataset: 'trips'\n",
      "  Shape: 20304 rows × 6 columns\n",
      "  Cols:  route_id, trip_id, service_id, trip_headsign, direction_id, shape_id\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Aggregated_Ridership_Oct_21_2024_Evening'\n",
      "  Shape: 424 rows × 2 columns\n",
      "  Cols:  station_complex_id, ridership\n",
      "--------------------\n",
      "Dataset: 'agency'\n",
      "  Shape: 1 rows × 6 columns\n",
      "  Cols:  agency_id, agency_name, agency_url, agency_timezone, agency_lang, agency_phone\n",
      "--------------------\n",
      "Dataset: 'stop_times'\n",
      "  Shape: 562597 rows × 5 columns\n",
      "  Cols:  trip_id, stop_id, arrival_time, departure_time, stop_sequence\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Hourly_Ridership__Oct_21_2024_Morning'\n",
      "  Shape: 15549 rows × 12 columns\n",
      "  Cols:  transit_timestamp, transit_mode, station_complex_id, station_complex, borough, payment_method, fare_class_category, ridership, transfers, latitude, longitude, Georeference\n",
      "--------------------\n",
      "Dataset: 'stop_routes'\n",
      "  Shape: 475 rows × 3 columns\n",
      "  Cols:  stop_id, stop_name, routes_at_stop\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Aggregated_Ridership_Oct_21_2024_Morning'\n",
      "  Shape: 424 rows × 2 columns\n",
      "  Cols:  station_complex_id, ridership\n",
      "--------------------\n",
      "Dataset: 'shapes'\n",
      "  Shape: 149834 rows × 4 columns\n",
      "  Cols:  shape_id, shape_pt_sequence, shape_pt_lat, shape_pt_lon\n",
      "--------------------\n",
      "Dataset: 'edges_by_route'\n",
      "  Shape: 1039 rows × 8 columns\n",
      "  Cols:  edge_idx, from_idx, to_idx, route_idx, from_stop_id, to_stop_id, route_short_name, count\n",
      "--------------------\n",
      "Dataset: 'calendar'\n",
      "  Shape: 3 rows × 10 columns\n",
      "  Cols:  service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date\n",
      "--------------------\n",
      "Dataset: 'evening_4to8_with_gtfs'\n",
      "  Shape: 475 rows × 3 columns\n",
      "  Cols:  station_complex_id, ridership, GTFS Stop ID\n",
      "--------------------\n",
      "Dataset: 'calendar_dates'\n",
      "  Shape: 8 rows × 3 columns\n",
      "  Cols:  service_id, date, exception_type\n",
      "--------------------\n",
      "Dataset: 'nodes'\n",
      "  Shape: 475 rows × 5 columns\n",
      "  Cols:  node_idx, stop_id, stop_name, stop_lon, stop_lat\n",
      "--------------------\n",
      "Dataset: 'generated_graphs_routes'\n",
      "  Shape: 26 rows × 2 columns\n",
      "  Cols:  route_idx, route_short_name\n",
      "--------------------\n",
      "Dataset: 'linelength'\n",
      "  Shape: 26 rows × 4 columns\n",
      "  Cols:  route_idx, route_short_name, route_length(mi), route_stations(rush_hour)\n",
      "--------------------\n",
      "Dataset: 'morning_6to10_with_gtfs'\n",
      "  Shape: 475 rows × 3 columns\n",
      "  Cols:  station_complex_id, ridership, GTFS Stop ID\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Hourly_Ridership__Oct_21_2024_Evening'\n",
      "  Shape: 15311 rows × 12 columns\n",
      "  Cols:  transit_timestamp, transit_mode, station_complex_id, station_complex, borough, payment_method, fare_class_category, ridership, transfers, latitude, longitude, Georeference\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Stations_20251204'\n",
      "  Shape: 496 rows × 19 columns\n",
      "  Cols:  GTFS Stop ID, Station ID, Complex ID, Division, Line, Stop Name, Borough, CBD, Daytime Routes, Structure, GTFS Latitude, GTFS Longitude, North Direction Label, South Direction Label, ADA, ADA Northbound, ADA Southbound, ADA Notes, Georeference\n",
      "--------------------\n",
      "Dataset: 'linecapacity'\n",
      "  Shape: 26 rows × 5 columns\n",
      "  Cols:  route_idx, route_short_name, cars_per_train, rush_hour_capacity_per_car, total_rush_hour_capacity\n",
      "--------------------\n",
      "Dataset: 'transfers'\n",
      "  Shape: 613 rows × 4 columns\n",
      "  Cols:  from_stop_id, to_stop_id, transfer_type, min_transfer_time\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\stops.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\transfers.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\trips.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\edges_by_route.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\nodes.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\nodes_with_ridership.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\routes.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\stop_routes.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\MTA_Subway_Aggregated_Ridership_Oct_21_2024_Evening.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\MTA_Subway_Aggregated_Ridership_Oct_21_2024_Morning.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\evening_4to8_with_gtfs.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\morning_6to10_with_gtfs.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully loaded 23 datasets.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    load_repo_data(repo_path::String)\n",
    "\n",
    "Recursively walks through the specified repository path, identifies all `.csv` and `.txt` files\n",
    "(even in subfolders), and parses them into a Dictionary of DataFrames.\n",
    "\n",
    "# Arguments\n",
    "- `repo_path::String`: The local path to the cloned repository.\n",
    "\n",
    "# Returns\n",
    "- `Dict{String, DataFrame}`: A dictionary where keys are unique filenames and values are DataFrames.\n",
    "\"\"\"\n",
    "function load_repo_data(repo_path::String)\n",
    "    # Dictionary to store the parsed data\n",
    "    data_store = Dict{String, DataFrame}()\n",
    "    \n",
    "    # CSV options (assume headers exist)\n",
    "    csv_options = (header=true, stringtype=String)\n",
    "\n",
    "    if !isdir(repo_path)\n",
    "        @error \"Directory not found: $repo_path\"\n",
    "        return data_store\n",
    "    end\n",
    "\n",
    "    @info \"Recursively scanning directory: $repo_path\"\n",
    "    \n",
    "    files_found = 0\n",
    "    \n",
    "    # walkdir allows us to search subdirectories (e.g., /data, /src)\n",
    "    for (root, dirs, files) in walkdir(repo_path)\n",
    "        for file in files\n",
    "            # Check for valid extensions\n",
    "            if endswith(lowercase(file), \".csv\") || endswith(lowercase(file), \".txt\")\n",
    "                \n",
    "                files_found += 1\n",
    "                full_path = joinpath(root, file)\n",
    "                dataset_name = splitext(file)[1]\n",
    "                \n",
    "                # Handle duplicate filenames in different folders by appending parent folder name\n",
    "                if haskey(data_store, dataset_name)\n",
    "                    parent_folder = basename(root)\n",
    "                    dataset_name = \"$(parent_folder)_$(dataset_name)\"\n",
    "                end\n",
    "\n",
    "                try\n",
    "                    @info \"Parsing: $full_path\"\n",
    "                    df = CSV.read(full_path, DataFrame; csv_options...)\n",
    "                    data_store[dataset_name] = df\n",
    "                catch e\n",
    "                    # Only warn, don't crash, if a file is malformed\n",
    "                    @warn \"Skipping $file: Unable to parse as CSV table.\"\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if files_found == 0\n",
    "        @warn \"No CSV or TXT files were found in $repo_path or its subdirectories.\"\n",
    "        @info \"Current working directory contains: $(readdir(repo_path))\"\n",
    "    else\n",
    "        @info \"Successfully loaded $(length(data_store)) datasets.\"\n",
    "    end\n",
    "\n",
    "    return data_store\n",
    "end\n",
    "\"\"\"\n",
    "    summarize_data(data::Dict{String, DataFrame})\n",
    "\n",
    "Prints a brief summary of the loaded datasets.\n",
    "\"\"\"\n",
    "function summarize_data(data::Dict{String, DataFrame})\n",
    "    println(\"\\n--- Data Summary ---\")\n",
    "    for (name, df) in data\n",
    "        println(\"Dataset: '$name'\")\n",
    "        println(\"  Shape: $(nrow(df)) rows × $(ncol(df)) columns\")\n",
    "        println(\"  Cols:  $(join(names(df), \", \"))\")\n",
    "        println(\"--------------------\")\n",
    "    end\n",
    "end\n",
    "\n",
    "repo_path = \".\" \n",
    "\n",
    "# 2. Load the data\n",
    "\n",
    "subway_data = load_repo_data(repo_path)\n",
    "\n",
    "# 3. Print summary\n",
    "summarize_data(subway_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf061a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7ef30-81f9-4815-989e-df69802ae1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff59c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using Gurobi\n",
    "\n",
    "\"\"\"\n",
    "    build_subway_model(V, E_track, E_transfer, L, L_ij; kwargs...) -> model\n",
    "\n",
    "Builds the Gurobi/JuMP model for the subway optimization problem with:\n",
    "- track edges E_track\n",
    "- transfer edges E_transfer\n",
    "- line set L\n",
    "- per-edge line sets L_ij\n",
    "\n",
    "This ONLY builds the optimizer; it does not read any CSVs.\n",
    "You must construct the sets and parameter dictionaries before calling it.\n",
    "\n",
    "Arguments\n",
    "---------\n",
    "V::Vector{Station}                # stations (any index type: Int, String, etc.)\n",
    "E_track::Vector{Tuple{Station,Station}}\n",
    "E_transfer::Vector{Tuple{Station,Station}}\n",
    "L::Vector{Line}\n",
    "L_ij::Dict{Tuple{Station,Station},Vector{Line}}\n",
    "\n",
    "Keyword parameters (all REQUIRED)\n",
    "---------------------------------\n",
    "s::Dict{Station,Float64}                             # s_i\n",
    "t::Dict{Tuple{Station,Station,Line},Float64}         # t_{ijℓ}\n",
    "t_tr::Dict{Tuple{Station,Station},Float64}           # t^{tr}_{ij}\n",
    "C_train::Float64                                     # C_train\n",
    "Δ::Float64                                           # Δ\n",
    "τ::Dict{Line,Float64}                                # τ_ℓ\n",
    "energy::Dict{Line,Float64}                           # energy_ℓ\n",
    "T_max::Float64                                       # T_max\n",
    "β::Float64                                           # β\n",
    "γ::Float64                                           # γ\n",
    "λ::Float64                                           # λ ∈ [0,1]\n",
    "\n",
    "Optional\n",
    "--------\n",
    "shared_track_constraint::Bool = true\n",
    "\n",
    "Returns\n",
    "-------\n",
    "::JuMP.Model (with Gurobi as optimizer)\n",
    "\"\"\"\n",
    "function build_subway_model(\n",
    "    V,\n",
    "    E_track,\n",
    "    E_transfer,\n",
    "    L,\n",
    "    L_ij;\n",
    "    s,\n",
    "    t,\n",
    "    t_tr,\n",
    "    C_train,\n",
    "    Δ,\n",
    "    τ,\n",
    "    energy,\n",
    "    T_max,\n",
    "    β,\n",
    "    γ,\n",
    "    λ,\n",
    "    shared_track_constraint::Bool = true,\n",
    ")\n",
    "\n",
    "    # -------------------------\n",
    "    # Create model\n",
    "    # -------------------------\n",
    "    model = Model(Gurobi.Optimizer)\n",
    "\n",
    "    # -------------------------\n",
    "    # Index sets for variables\n",
    "    # -------------------------\n",
    "    # Triplets (i,j,ℓ) where ℓ actually serves edge (i,j)\n",
    "    track_triplets = Tuple{eltype(V),eltype(V),eltype(L)}[]\n",
    "    for (i, j) in E_track\n",
    "        lines_ij = get(L_ij, (i, j), Vector{eltype(L)}())\n",
    "        for ℓ in lines_ij\n",
    "            push!(track_triplets, (i, j, ℓ))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # -------------------------\n",
    "    # Decision variables\n",
    "    # -------------------------\n",
    "\n",
    "    # x_{ijℓ} ≥ 0 : passenger flow on track edge (i,j) via line ℓ\n",
    "    @variable(model, x[track_triplets] >= 0)\n",
    "\n",
    "    # y_{ij} ≥ 0 : passenger flow on transfer edge (i,j)\n",
    "    @variable(model, y[E_transfer] >= 0)\n",
    "\n",
    "    # f_ℓ ≥ 0 : train frequency on line ℓ\n",
    "    @variable(model, f[L] >= 0)\n",
    "\n",
    "    # overflow_{ijℓ} ≥ 0\n",
    "    @variable(model, overflow[track_triplets] >= 0)\n",
    "\n",
    "    # -------------------------\n",
    "    # Constraints\n",
    "    # -------------------------\n",
    "\n",
    "    # (1) Flow conservation at each station i ∈ V\n",
    "    @constraint(model, [i in V], begin\n",
    "        # outgoing track flows\n",
    "        out_track = sum(\n",
    "            x[(i, j, ℓ)]\n",
    "            for (ii, j, ℓ) in track_triplets\n",
    "            if ii == i\n",
    "        )\n",
    "\n",
    "        # outgoing transfer flows\n",
    "        out_transfer = sum(\n",
    "            y[(i, j)]\n",
    "            for (u, j) in E_transfer\n",
    "            if u == i\n",
    "        )\n",
    "\n",
    "        # incoming track flows\n",
    "        in_track = sum(\n",
    "            x[(k, i, ℓ)]\n",
    "            for (k, jj, ℓ) in track_triplets\n",
    "            if jj == i\n",
    "        )\n",
    "\n",
    "        # incoming transfer flows\n",
    "        in_transfer = sum(\n",
    "            y[(k, i)]\n",
    "            for (k, v) in E_transfer\n",
    "            if v == i\n",
    "        )\n",
    "\n",
    "        out_track + out_transfer - in_track - in_transfer == s[i]\n",
    "    end)\n",
    "\n",
    "    # (2) Capacity constraints on track edges\n",
    "    @constraint(model, [triplet in track_triplets], begin\n",
    "        (i, j, ℓ) = triplet\n",
    "        x[triplet] - overflow[triplet] <= C_train * f[ℓ] * Δ\n",
    "    end)\n",
    "\n",
    "    # (3) Optional shared physical track constraints\n",
    "    #     Sum over lines on each (i,j)\n",
    "    if shared_track_constraint\n",
    "        @constraint(model, [e in E_track], begin\n",
    "            i, j = e\n",
    "            lines_ij = get(L_ij, (i, j), Vector{eltype(L)}())\n",
    "            if isempty(lines_ij)\n",
    "                0.0 <= 0.0   # dummy constraint if edge has no lines\n",
    "            else\n",
    "                sum(x[(i, j, ℓ)] - overflow[(i, j, ℓ)] for ℓ in lines_ij) <=\n",
    "                sum(C_train * f[ℓ] * Δ for ℓ in lines_ij)\n",
    "            end\n",
    "        end)\n",
    "    end\n",
    "\n",
    "    # (4) Fleet / energy limit:\n",
    "    @constraint(model,\n",
    "        sum(f[ℓ] * τ[ℓ] for ℓ in L) <= T_max\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Objective\n",
    "    # -------------------------\n",
    "\n",
    "    # Passenger time part:\n",
    "    # sum_{(i,j)∈E_track, ℓ∈L_ij} t_{ijℓ} x_{ijℓ}\n",
    "    # + sum_{(i,j)∈E_transfer} t^{tr}_{ij} y_{ij}\n",
    "    passenger_time_expr =\n",
    "        sum(t[(i, j, ℓ)] * x[(i, j, ℓ)] for (i, j, ℓ) in track_triplets) +\n",
    "        sum(t_tr[(i, j)] * y[(i, j)] for (i, j) in E_transfer)\n",
    "\n",
    "    # Overflow penalty: β * sum overflow_{ijℓ}\n",
    "    overflow_expr = β * sum(overflow[triplet] for triplet in track_triplets)\n",
    "\n",
    "    # Energy use: γ * sum energy_ℓ * f_ℓ\n",
    "    energy_expr = γ * sum(energy[ℓ] * f[ℓ] for ℓ in L)\n",
    "\n",
    "    @objective(model, Min,\n",
    "        (1 - λ) * (passenger_time_expr + overflow_expr) +\n",
    "        λ * energy_expr\n",
    "    )\n",
    "\n",
    "    return model\n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
