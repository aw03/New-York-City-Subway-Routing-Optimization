{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a3820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Gurobi, CSV, DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe509ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRecursively scanning directory: .\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\MTA_Subway_Hourly_Ridership__Oct_21_2024_Evening.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\MTA_Subway_Hourly_Ridership__Oct_21_2024_Morning.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\MTA_Subway_Stations_20251204.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\agency.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\calendar.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\calendar_dates.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\linecapacity.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\linelength.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\routes.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\shapes.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\stop_times.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Summary ---\n",
      "Dataset: 'routes'\n",
      "  Shape: 29 rows × 10 columns\n",
      "  Cols:  route_id, agency_id, route_short_name, route_long_name, route_desc, route_type, route_url, route_color, route_text_color, route_sort_order\n",
      "--------------------\n",
      "Dataset: 'nodes_with_ridership'\n",
      "  Shape: 475 rows × 9 columns\n",
      "  Cols:  node_idx, stop_id, stop_name, stop_lon, stop_lat, station_complex_id, ridership_morning, ridership_evening, net_ridership\n",
      "--------------------\n",
      "Dataset: 'stops'\n",
      "  Shape: 1488 rows × 6 columns\n",
      "  Cols:  stop_id, stop_name, stop_lat, stop_lon, location_type, parent_station\n",
      "--------------------\n",
      "Dataset: 'trips'\n",
      "  Shape: 20304 rows × 6 columns\n",
      "  Cols:  route_id, trip_id, service_id, trip_headsign, direction_id, shape_id\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Aggregated_Ridership_Oct_21_2024_Evening'\n",
      "  Shape: 424 rows × 2 columns\n",
      "  Cols:  station_complex_id, ridership\n",
      "--------------------\n",
      "Dataset: 'agency'\n",
      "  Shape: 1 rows × 6 columns\n",
      "  Cols:  agency_id, agency_name, agency_url, agency_timezone, agency_lang, agency_phone\n",
      "--------------------\n",
      "Dataset: 'stop_times'\n",
      "  Shape: 562597 rows × 5 columns\n",
      "  Cols:  trip_id, stop_id, arrival_time, departure_time, stop_sequence\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Hourly_Ridership__Oct_21_2024_Morning'\n",
      "  Shape: 15549 rows × 12 columns\n",
      "  Cols:  transit_timestamp, transit_mode, station_complex_id, station_complex, borough, payment_method, fare_class_category, ridership, transfers, latitude, longitude, Georeference\n",
      "--------------------\n",
      "Dataset: 'stop_routes'\n",
      "  Shape: 475 rows × 3 columns\n",
      "  Cols:  stop_id, stop_name, routes_at_stop\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Aggregated_Ridership_Oct_21_2024_Morning'\n",
      "  Shape: 424 rows × 2 columns\n",
      "  Cols:  station_complex_id, ridership\n",
      "--------------------\n",
      "Dataset: 'shapes'\n",
      "  Shape: 149834 rows × 4 columns\n",
      "  Cols:  shape_id, shape_pt_sequence, shape_pt_lat, shape_pt_lon\n",
      "--------------------\n",
      "Dataset: 'edges_by_route'\n",
      "  Shape: 1039 rows × 8 columns\n",
      "  Cols:  edge_idx, from_idx, to_idx, route_idx, from_stop_id, to_stop_id, route_short_name, count\n",
      "--------------------\n",
      "Dataset: 'calendar'\n",
      "  Shape: 3 rows × 10 columns\n",
      "  Cols:  service_id, monday, tuesday, wednesday, thursday, friday, saturday, sunday, start_date, end_date\n",
      "--------------------\n",
      "Dataset: 'evening_4to8_with_gtfs'\n",
      "  Shape: 475 rows × 3 columns\n",
      "  Cols:  station_complex_id, ridership, GTFS Stop ID\n",
      "--------------------\n",
      "Dataset: 'calendar_dates'\n",
      "  Shape: 8 rows × 3 columns\n",
      "  Cols:  service_id, date, exception_type\n",
      "--------------------\n",
      "Dataset: 'nodes'\n",
      "  Shape: 475 rows × 5 columns\n",
      "  Cols:  node_idx, stop_id, stop_name, stop_lon, stop_lat\n",
      "--------------------\n",
      "Dataset: 'generated_graphs_routes'\n",
      "  Shape: 26 rows × 2 columns\n",
      "  Cols:  route_idx, route_short_name\n",
      "--------------------\n",
      "Dataset: 'linelength'\n",
      "  Shape: 26 rows × 4 columns\n",
      "  Cols:  route_idx, route_short_name, route_length(mi), route_stations(rush_hour)\n",
      "--------------------\n",
      "Dataset: 'morning_6to10_with_gtfs'\n",
      "  Shape: 475 rows × 3 columns\n",
      "  Cols:  station_complex_id, ridership, GTFS Stop ID\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Hourly_Ridership__Oct_21_2024_Evening'\n",
      "  Shape: 15311 rows × 12 columns\n",
      "  Cols:  transit_timestamp, transit_mode, station_complex_id, station_complex, borough, payment_method, fare_class_category, ridership, transfers, latitude, longitude, Georeference\n",
      "--------------------\n",
      "Dataset: 'MTA_Subway_Stations_20251204'\n",
      "  Shape: 496 rows × 19 columns\n",
      "  Cols:  GTFS Stop ID, Station ID, Complex ID, Division, Line, Stop Name, Borough, CBD, Daytime Routes, Structure, GTFS Latitude, GTFS Longitude, North Direction Label, South Direction Label, ADA, ADA Northbound, ADA Southbound, ADA Notes, Georeference\n",
      "--------------------\n",
      "Dataset: 'linecapacity'\n",
      "  Shape: 26 rows × 5 columns\n",
      "  Cols:  route_idx, route_short_name, cars_per_train, rush_hour_capacity_per_car, total_rush_hour_capacity\n",
      "--------------------\n",
      "Dataset: 'transfers'\n",
      "  Shape: 613 rows × 4 columns\n",
      "  Cols:  from_stop_id, to_stop_id, transfer_type, min_transfer_time\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\stops.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\transfers.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\datasets\\trips.txt\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\edges_by_route.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\nodes.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\nodes_with_ridership.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\routes.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_graphs\\stop_routes.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\MTA_Subway_Aggregated_Ridership_Oct_21_2024_Evening.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\MTA_Subway_Aggregated_Ridership_Oct_21_2024_Morning.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\evening_4to8_with_gtfs.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mParsing: .\\generated_turnstile_data\\morning_6to10_with_gtfs.csv\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully loaded 23 datasets.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    load_repo_data(repo_path::String)\n",
    "\n",
    "Recursively walks through the specified repository path, identifies all `.csv` and `.txt` files\n",
    "(even in subfolders), and parses them into a Dictionary of DataFrames.\n",
    "\n",
    "# Arguments\n",
    "- `repo_path::String`: The local path to the cloned repository.\n",
    "\n",
    "# Returns\n",
    "- `Dict{String, DataFrame}`: A dictionary where keys are unique filenames and values are DataFrames.\n",
    "\"\"\"\n",
    "function load_repo_data(repo_path::String)\n",
    "    # Dictionary to store the parsed data\n",
    "    data_store = Dict{String, DataFrame}()\n",
    "    \n",
    "    # CSV options (assume headers exist)\n",
    "    csv_options = (header=true, stringtype=String)\n",
    "\n",
    "    if !isdir(repo_path)\n",
    "        @error \"Directory not found: $repo_path\"\n",
    "        return data_store\n",
    "    end\n",
    "\n",
    "    @info \"Recursively scanning directory: $repo_path\"\n",
    "    \n",
    "    files_found = 0\n",
    "    \n",
    "    # walkdir allows us to search subdirectories (e.g., /data, /src)\n",
    "    for (root, dirs, files) in walkdir(repo_path)\n",
    "        for file in files\n",
    "            # Check for valid extensions\n",
    "            if endswith(lowercase(file), \".csv\") || endswith(lowercase(file), \".txt\")\n",
    "                \n",
    "                files_found += 1\n",
    "                full_path = joinpath(root, file)\n",
    "                dataset_name = splitext(file)[1]\n",
    "                \n",
    "                # Handle duplicate filenames in different folders by appending parent folder name\n",
    "                if haskey(data_store, dataset_name)\n",
    "                    parent_folder = basename(root)\n",
    "                    dataset_name = \"$(parent_folder)_$(dataset_name)\"\n",
    "                end\n",
    "\n",
    "                try\n",
    "                    @info \"Parsing: $full_path\"\n",
    "                    df = CSV.read(full_path, DataFrame; csv_options...)\n",
    "                    data_store[dataset_name] = df\n",
    "                catch e\n",
    "                    # Only warn, don't crash, if a file is malformed\n",
    "                    @warn \"Skipping $file: Unable to parse as CSV table.\"\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if files_found == 0\n",
    "        @warn \"No CSV or TXT files were found in $repo_path or its subdirectories.\"\n",
    "        @info \"Current working directory contains: $(readdir(repo_path))\"\n",
    "    else\n",
    "        @info \"Successfully loaded $(length(data_store)) datasets.\"\n",
    "    end\n",
    "\n",
    "    return data_store\n",
    "end\n",
    "\"\"\"\n",
    "    summarize_data(data::Dict{String, DataFrame})\n",
    "\n",
    "Prints a brief summary of the loaded datasets.\n",
    "\"\"\"\n",
    "function summarize_data(data::Dict{String, DataFrame})\n",
    "    println(\"\\n--- Data Summary ---\")\n",
    "    for (name, df) in data\n",
    "        println(\"Dataset: '$name'\")\n",
    "        println(\"  Shape: $(nrow(df)) rows × $(ncol(df)) columns\")\n",
    "        println(\"  Cols:  $(join(names(df), \", \"))\")\n",
    "        println(\"--------------------\")\n",
    "    end\n",
    "end\n",
    "\n",
    "repo_path = \".\" \n",
    "\n",
    "# 2. Load the data\n",
    "\n",
    "subway_data = load_repo_data(repo_path)\n",
    "\n",
    "# 3. Print summary\n",
    "summarize_data(subway_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf061a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7ef30-81f9-4815-989e-df69802ae1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
